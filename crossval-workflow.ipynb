{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Workflow Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of information, code and discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This notebook follows the 5 step process presented in the Chris Deotte \"How to compete with GPUs Workshop\" [here][1].\n",
    "2. Triple stratified KFold TFRecords used for image data is explained [here][2].\n",
    "3. Some code sections have been reused from AgentAuers' notebook [here][3]\n",
    "4. The advantage of using different input sizes is discussed [here][4]\n",
    "5. Use external data by changing the variables `INC2019` and `INC2018`.These variables respectively indicate whether to load last year 2019 data and/or year 2018 + 2017 data. These datasets are discussed [here][5]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/how-to-compete-with-gpus-workshop\n",
    "[2]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/165526\n",
    "[3]: https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once\n",
    "[4]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147\n",
    "[5]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/164910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Initiatialise environment and import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Use if running in Kaggle environment\n",
    "#!pip install -q efficientnet >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KaggleDatasets if running in Kaggle environment\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "import os, re, math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "* DEVICE - is GPU or TPU\n",
    "* SEED - a different seed produces a different triple stratified kfold split.\n",
    "* FOLDS - number of folds. Best set to 3, 5, or 15 for conistent number of train and validation samples across folds\n",
    "* IMG_SIZES - These are the image sizes to use each fold\n",
    "* INC2019 - This includes the new half of the 2019 competition data. The second half of the 2019 data is the comp data from 2018 plus 2017\n",
    "* INC2018 - This includes the second half of the 2019 competition data which is the comp data from 2018 plus 2017\n",
    "* BATCH_SIZES - These are batch sizes for each fold. For maximum speed, it is best to use the largest batch size your GPU or TPU allows.\n",
    "* EPOCHS - These are maximum epochs. Note that each fold, the best epoch model is saved and used. So if epochs is too large, it won't matter. Consider early stopping in Callbacks.\n",
    "* EFF_NETS - These are the EfficientNets to use each fold. The number refers to the B. So a number of `0` refers to EfficientNetB0, and `1` refers to EfficientNetB1, etc.\n",
    "* WGTS - this should be `1/FOLDS` for each fold. This is the weight when ensembling the folds to predict the test set. If you want a weird ensemble, you can use different weights.\n",
    "* TTA - test time augmentation. Each test image is randomly augmented and predicted TTA times and the average prediction is used. TTA is also applied to OOF during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "ENV = \"LOCAL\" #or \"KAGGLE\"\n",
    "\n",
    "# DEFAULT TO TPU TO ENSURE KAGGLE TPU COMPATIABILITY\n",
    "# https://www.kaggle.com/docs/tpu\n",
    "DEVICE = \"TPU\" #or \"GPU\"\n",
    "\n",
    "# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n",
    "SEED = 1\n",
    "\n",
    "# NUMBER OF FOLDS. USE 3, 5, OR 15 \n",
    "FOLDS = 5\n",
    "\n",
    "# WHICH IMAGE SIZES TO LOAD EACH FOLD\n",
    "# CHOOSE 128, 192, 256, 384, 512, 768 \n",
    "IMG_SIZES = [384]*FOLDS\n",
    "\n",
    "# INCLUDE OLD COMP DATA? YES=1 NO=0\n",
    "INC2019 = [0]*FOLDS\n",
    "INC2018 = [0]*FOLDS\n",
    "\n",
    "# BATCH SIZE AND EPOCHS\n",
    "# TRY 8, 16, 32, 64, 128, 256. REDUCE IF OOM ERROR, HIGHER FOR TPUS\n",
    "BATCH_SIZES = [16]*FOLDS\n",
    "EPOCHS = [15]*FOLDS\n",
    "\n",
    "# WHICH EFFICIENTNET B? TO USE\n",
    "EFF_NETS = [4]*FOLDS\n",
    "\n",
    "# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\n",
    "WGTS = [1/FOLDS]*FOLDS\n",
    "\n",
    "# TEST TIME AUGMENTATION STEPS\n",
    "TTA = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure environment to use TPUs, Multiple GPUs, Single GPU or just CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Could not connect to TPU\n",
      "Using default strategy for CPU and single GPU\n",
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocess and file handling\n",
    "Preprocess has already been done and saved to TFRecords. Here we choose which size to load. We can use either 128x128, 192x192, 256x256, 384x384, 512x512, 768x768 by changing the `IMG_SIZES` variable in the preceeding code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file locations\n",
    "if ENV == 'KAGGLE':\n",
    "    #Use GCS if running in Kaggle environment\n",
    "    PATH = [None]*FOLDS; PATH2 = [None]*FOLDS\n",
    "    for i,k in enumerate(IMG_SIZES):\n",
    "        PATH[i] = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(k,k))\n",
    "        PATH2[i] = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(k,k))\n",
    "\n",
    "    files_train = np.sort(np.array(tf.io.gfile.glob(PATH[0] + '/train*.tfrec')))\n",
    "    files_test  = np.sort(np.array(tf.io.gfile.glob(PATH[0] + '/test*.tfrec')))\n",
    "\n",
    "if ENV == 'LOCAL':  \n",
    "    # Use LDS if running in local environment\n",
    "    PATH = [None]*FOLDS; PATH2 = [None]*FOLDS\n",
    "    for i,k in enumerate(IMG_SIZES):\n",
    "        PATH[i] = f\"./siim-isic-melanoma-classification/tfrecords{k}\"\n",
    "        PATH2[i] = f\"./siim-isic-melanoma-classification/tfrecords{k}Ext\"\n",
    "\n",
    "    files_train = np.sort(np.array(tf.io.gfile.glob(PATH[0] + '/train*.tfrec')))\n",
    "    files_test  = np.sort(np.array(tf.io.gfile.glob(PATH[0] + '/test*.tfrec')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file locations\n",
    "if ENV == 'KAGGLE':\n",
    "    LOG_PATH = ''\n",
    "    CV_OOF_PREDS_PATH = ''\n",
    "    CV_TEST_PREDS_PATH = ''\n",
    "    CV_FOLDS_PATH = ''\n",
    "\n",
    "if ENV == 'LOCAL':\n",
    "    LOG_PATH = './logs/'\n",
    "    CV_OOF_PREDS_PATH = './cv_oof_preds/'\n",
    "    CV_TEST_PREDS_PATH = './cv_test_preds/'\n",
    "    CV_FOLDS_PATH = './cv_folds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling functions\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['target']\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_name):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['image_name'] if return_image_name else 0\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream datasets into model when training, validating and predicting\n",
    "def get_dataset(files, augment = False, shuffle = False, repeat = False, \n",
    "                labeled=True, return_image_names=True, batch_size=16, dim=256):\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "        \n",
    "    if labeled: \n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n",
    "                    num_parallel_calls=AUTO)      \n",
    "    \n",
    "    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), \n",
    "                                               imgname_or_label), \n",
    "                num_parallel_calls=AUTO)\n",
    "    \n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Augmentation\n",
    "This notebook uses rotation, sheer, zoom, shift augmentation. This notebook also uses horizontal flip, saturation, contrast, brightness augmentation similar to last years winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "ROT_ = 180.0\n",
    "SHR_ = 2.0\n",
    "HZOOM_ = 8.0\n",
    "WZOOM_ = 8.0\n",
    "HSHIFT_ = 8.0\n",
    "WSHIFT_ = 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, DIM=256):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = SHR_ * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n",
    "    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])\n",
    "\n",
    "def prepare_image(img, augment=True, dim=256):    \n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if augment:\n",
    "        img = transform(img,DIM=dim)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1)\n",
    "                      \n",
    "    img = tf.reshape(img, [dim,dim, 3])\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build Model\n",
    "This is a common model architecute. Consider experimenting with different backbones, custom heads, losses, and optimizers. Also consider inputing meta features into your CNN. Also consider different models to provide diversity of predictions which may benefit the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 384, 384, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b4 (Functional) (None, 12, 12, 1792)      17673816  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1793      \n",
      "=================================================================\n",
      "Total params: 17,675,609\n",
      "Trainable params: 17,550,409\n",
      "Non-trainable params: 125,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7faad82a7e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "\n",
    "def build_model(dim=128, ef=0, v=0):\n",
    "    inputs = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    \n",
    "    base = EFNS[ef](input_shape=(dim,dim,3),weights='noisy-student',include_top=False)\n",
    "    \n",
    "    x = base(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n",
    "    \n",
    "    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n",
    "    \n",
    "    if v:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Visualise model in use\n",
    "build_model(dim=IMG_SIZES[0],ef=EFF_NETS[0],v=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Schedule\n",
    "Thism is a common train schedule for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time. Consider changing the schedule and/or learning rates. Note how the learning rate max is larger with larger batches sizes. This is a good practice to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_CFG = dict(\n",
    "    lr_start   = 0.000005,\n",
    "    lr_max     = 0.00000125 * REPLICAS * BATCH_SIZES[0],\n",
    "    lr_min     = 0.000001,\n",
    "    lr_ramp_ep = 5,\n",
    "    lr_sus_ep  = 0,\n",
    "    lr_decay   = 0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(ts_cfg, batch_size=8):\n",
    "    lr_start   = ts_cfg['lr_start']\n",
    "    lr_max     = ts_cfg['lr_max']\n",
    "    lr_min     = ts_cfg['lr_min']\n",
    "    lr_ramp_ep = ts_cfg['lr_ramp_ep']\n",
    "    lr_sus_ep  = ts_cfg['lr_sus_ep']\n",
    "    lr_decay   = ts_cfg['lr_decay']\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    \n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Training Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick test and visualisation of learning rate schedule\n",
    "# Important not to have a high learning rate which would destroy the pre-trained parameters\n",
    "lr_start   = TS_CFG['lr_start']\n",
    "lr_max     = TS_CFG['lr_max']\n",
    "lr_min     = TS_CFG['lr_min']\n",
    "lr_ramp_ep = TS_CFG['lr_ramp_ep']\n",
    "lr_sus_ep  = TS_CFG['lr_sus_ep']\n",
    "lr_decay   = TS_CFG['lr_decay']\n",
    "\n",
    "def lrfn_sched(epoch):\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "# Visualise learning rate schedule\n",
    "rng = [i for i in range(100)]\n",
    "y = [lrfn_sched(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "plt.xlabel('epoch', size=14); plt.ylabel('learning rate', size=14)\n",
    "plt.title('Training Schedule', size=16); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variables `VERBOSE` and `DISPLOY_PLOT` below to determine what output you want displayed. The variable `VERBOSE=1 or 2` will display the training and validation loss and auc for each epoch as text. The variable `DISPLAY_PLOT` shows this information as a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "#### FOLD 1\n",
      "#### Image Size 384 with EfficientNet B4 and batch_size 16\n",
      "#########################\n",
      "Training...\n",
      "Epoch 1/15\n",
      "   2/1635 [..............................] - ETA: 7:01 - loss: 0.6277 - auc: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1951s vs `on_train_batch_end` time: 0.3208s). Check your callbacks.\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.2288 - auc: 0.5224\n",
      "Epoch 00001: val_loss improved from inf to 0.17086, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 881s 539ms/step - loss: 0.2288 - auc: 0.5224 - val_loss: 0.1709 - val_auc: 0.7470\n",
      "Epoch 2/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1713 - auc: 0.7124\n",
      "Epoch 00002: val_loss improved from 0.17086 to 0.16859, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 880s 538ms/step - loss: 0.1713 - auc: 0.7124 - val_loss: 0.1686 - val_auc: 0.8173\n",
      "Epoch 3/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1733 - auc: 0.7719\n",
      "Epoch 00003: val_loss improved from 0.16859 to 0.16695, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 878s 537ms/step - loss: 0.1733 - auc: 0.7719 - val_loss: 0.1669 - val_auc: 0.8535\n",
      "Epoch 4/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1673 - auc: 0.8325\n",
      "Epoch 00004: val_loss improved from 0.16695 to 0.16548, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 877s 537ms/step - loss: 0.1673 - auc: 0.8325 - val_loss: 0.1655 - val_auc: 0.8621\n",
      "Epoch 5/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1677 - auc: 0.8216\n",
      "Epoch 00005: val_loss improved from 0.16548 to 0.16531, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 875s 535ms/step - loss: 0.1677 - auc: 0.8216 - val_loss: 0.1653 - val_auc: 0.8684\n",
      "Epoch 6/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1657 - auc: 0.8535\n",
      "Epoch 00006: val_loss improved from 0.16531 to 0.16447, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 877s 536ms/step - loss: 0.1657 - auc: 0.8535 - val_loss: 0.1645 - val_auc: 0.8868\n",
      "Epoch 7/15\n",
      "1635/1635 [==============================] - ETA: 0s - loss: 0.1664 - auc: 0.8633\n",
      "Epoch 00007: val_loss improved from 0.16447 to 0.16268, saving model to ./cv_folds/EB4-384-fold-0.h5\n",
      "1635/1635 [==============================] - 879s 537ms/step - loss: 0.1664 - auc: 0.8633 - val_loss: 0.1627 - val_auc: 0.8884\n",
      "Epoch 8/15\n",
      "1389/1635 [========================>.....] - ETA: 2:06 - loss: 0.1628 - auc: 0.8875"
     ]
    }
   ],
   "source": [
    "# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n",
    "VERBOSE = 1\n",
    "DISPLAY_PLOT = True\n",
    "\n",
    "skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n",
    "oof_pred = []\n",
    "oof_tar = []\n",
    "oof_val = []\n",
    "oof_names = []\n",
    "oof_folds = [] \n",
    "preds = np.zeros((count_data_items(files_test),1))\n",
    "\n",
    "best_val_auc_ls = []\n",
    "best_epoch_val_auc_ls = []\n",
    "\n",
    "best_val_loss_ls = []\n",
    "best_epoch_val_loss_ls = []\n",
    "\n",
    "\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\n",
    "    \n",
    "    # DISPLAY FOLD INFO\n",
    "    if DEVICE=='TPU':\n",
    "        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    print('#'*25); print('#### FOLD',fold+1)\n",
    "    print(f'#### Image Size {IMG_SIZES[fold]} with EfficientNet B{EFF_NETS[fold]} and batch_size {BATCH_SIZES[fold]*REPLICAS}')\n",
    "    \n",
    "    # CREATE TRAIN AND VALIDATION SUBSETS\n",
    "    files_train = tf.io.gfile.glob([PATH[fold] + '/train%.2i*.tfrec'%x for x in idxT])\n",
    "    if INC2019[fold]:\n",
    "        files_train += tf.io.gfile.glob([PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2+1])\n",
    "        print('#### Using 2019 external data')\n",
    "    if INC2018[fold]:\n",
    "        files_train += tf.io.gfile.glob([PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2])\n",
    "        print('#### Using 2018+2017 external data')\n",
    "    np.random.shuffle(files_train); print('#'*25)\n",
    "    files_valid = tf.io.gfile.glob([PATH[fold] + '/train%.2i*.tfrec'%x for x in idxV])\n",
    "    files_test = np.sort(np.array(tf.io.gfile.glob(PATH[fold] + '/test*.tfrec')))\n",
    "    \n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold],v=0)\n",
    "        \n",
    "    # CALLBACKS - SAVE BEST MODEL EACH FOLD ALSO UTILISE EARLY STOPPING TO ALLOW MORE EPOCHS\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{CV_FOLDS_PATH}EB{EFF_NETS[0]}-{IMG_SIZES[0]}-fold-{fold}.h5', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='min', save_freq='epoch')\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=VERBOSE, mode='auto')\n",
    "    callback_list = [sv,es,get_lr_callback(TS_CFG, BATCH_SIZES[fold])]\n",
    "    \n",
    "    # TRAIN\n",
    "    print('Training...')\n",
    "    history = model.fit(\n",
    "        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n",
    "                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n",
    "        epochs=EPOCHS[fold],\n",
    "        callbacks = callback_list, \n",
    "        steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\n",
    "        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n",
    "                repeat=False,dim=IMG_SIZES[fold]), #class_weight = {0:1,1:2},\n",
    "        verbose=VERBOSE\n",
    "    )\n",
    "    \n",
    "    # DETERMINE BEST SCORES AND EPOCHS\n",
    "    val_auc_ls = history.history['val_auc']\n",
    "    val_loss_ls = history.history['val_loss']\n",
    "    \n",
    "    best_val_auc = max(val_auc_ls)\n",
    "    best_val_loss = min(val_loss_ls)\n",
    "    best_val_auc_ls.append(best_val_auc)\n",
    "    best_val_loss_ls.append(best_val_loss)\n",
    "    \n",
    "    opt_epoch_auc = val_auc_ls.index(best_val_auc)\n",
    "    opt_epoch_loss = val_loss_ls.index(best_val_loss)\n",
    "    best_epoch_val_auc_ls.append(opt_epoch_auc)\n",
    "    best_epoch_val_loss_ls.append(opt_epoch_loss)\n",
    "    \n",
    "    #RELOAD BEST WEIGHTS AT BEST EPOCH\n",
    "    print('Loading best model...')\n",
    "    model.load_weights(f'{CV_FOLDS_PATH}EB{EFF_NETS[0]}-{IMG_SIZES[0]}-fold-{fold}.h5')\n",
    "    \n",
    "    # PREDICT OOF USING TTA\n",
    "    print('Predicting OOF with TTA...')\n",
    "    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n",
    "            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n",
    "    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n",
    "    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n",
    "    \n",
    "    # GET OOF TARGETS AND NAMES\n",
    "    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n",
    "            labeled=True, return_image_names=True)\n",
    "    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n",
    "    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n",
    "    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n",
    "                labeled=False, return_image_names=True)\n",
    "    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n",
    "    \n",
    "    # PREDICT TEST USING TTA\n",
    "    print('Predicting Test with TTA...')\n",
    "    ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n",
    "            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "    ct_test = count_data_items(files_test); STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n",
    "    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n",
    "    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n",
    "    \n",
    "    # REPORT RESULTS\n",
    "    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n",
    "    oof_val.append(np.max( history.history['val_auc'] ))\n",
    "    print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n",
    "    \n",
    "    COMPLETED_EPOCHS = len(history.history['loss'])\n",
    "    \n",
    "    # PLOT TRAINING\n",
    "    if DISPLAY_PLOT:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(np.arange(COMPLETED_EPOCHS),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
    "        plt.plot(np.arange(COMPLETED_EPOCHS),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n",
    "        x = np.argmax( history.history['val_auc'] )\n",
    "        y = np.max( history.history['val_auc'] )\n",
    "        xdist = plt.xlim()[1] - plt.xlim()[0]\n",
    "        ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "        plt.scatter(x,y,s=200,color='#1f77b4')\n",
    "        plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
    "        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n",
    "        plt.legend(loc=2)\n",
    "        plt2 = plt.gca().twinx()\n",
    "        plt2.plot(np.arange(COMPLETED_EPOCHS),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "        plt2.plot(np.arange(COMPLETED_EPOCHS),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
    "        x = np.argmin( history.history['val_loss'] )\n",
    "        y = np.min( history.history['val_loss'] )\n",
    "        ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "        plt.scatter(x,y,s=200,color='#d62728')\n",
    "        plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "        plt.ylabel('Loss',size=14)\n",
    "        plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n",
    "                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n",
    "        plt.legend(loc=3)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the optimal characteristics from CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_val_auc_mean = np.mean(np.array(best_epoch_val_auc_ls), axis=0)\n",
    "best_epoch_val_loss_mean = np.mean(np.array(best_epoch_val_loss_ls), axis=0)\n",
    "\n",
    "print(\"The best epoch value for AUC over the cv is: \",best_epoch_val_auc_mean)\n",
    "print(\"The best epoch value for loss over the cv is: \",best_epoch_val_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_val_auc_mean = np.mean(np.array(best_val_auc_ls), axis=0)\n",
    "cv_val_auc_std = np.std(np.array(best_val_auc_ls), axis=0)\n",
    "cv_val_loss_mean = np.mean(np.array(best_val_loss_ls), axis=0)\n",
    "\n",
    "print(f\"The mean AUC score across CV folds is {cv_val_auc_mean:.2f} and the standard deviation is {cv_val_auc_std:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate OOF AUC\n",
    "The OOF (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the OOF to determine what are the best weights to blend your models with. Choose weights that maximize OOF CV score when used to blend OOF. Then use those same weights to blend your test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE OVERALL OOF AUC\n",
    "oof = np.concatenate(oof_pred)\n",
    "true = np.concatenate(oof_tar);\n",
    "names = np.concatenate(oof_names)\n",
    "folds = np.concatenate(oof_folds)\n",
    "auc = roc_auc_score(true,oof)\n",
    "print('Overall OOF AUC with TTA = %.3f'%auc)\n",
    "\n",
    "# SAVE OOF TO DISK\n",
    "df_oof = pd.DataFrame(dict(\n",
    "    image_name = names, target=true, pred = oof, fold=folds))\n",
    "df_oof.to_csv(f'{CV_OOF_PREDS_PATH}EB{EFF_NETS[0]}-{IMG_SIZES[0]}-oof.csv',index=False)\n",
    "df_oof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Post process\n",
    "There are ways to modify predictions based on patient information to increase CV LB. You can experiment with that here on your OOF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log values from the cross validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV == 'LOCAL':\n",
    "    log_df = pd.read_csv('./logs/experiment_log.csv')\n",
    "    headers = list(log_df.columns.values)\n",
    "    \n",
    "if ENV == 'KAGGLE':\n",
    "    headers = ['Image Size',\n",
    "             'Model',\n",
    "             'Batch Size',\n",
    "             'TTA Loops',\n",
    "             'CV Folds',\n",
    "             'Mean CV AUC',\n",
    "             'Std CV AUC',\n",
    "             'OOF AUC with TTA',\n",
    "             'Mean CV Loss',\n",
    "             'Total Epochs',\n",
    "             'Best AUC Epoch',\n",
    "             'Best Loss Epoch',\n",
    "             'Kaggle LB AUC']\n",
    "    log_df = pd.DataFrame(columns = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_items = [IMG_SIZES[0],\n",
    "            str(f\"EFNB{EFF_NETS[0]}\"),\n",
    "            BATCH_SIZES[0],\n",
    "            TTA,\n",
    "            FOLDS,\n",
    "            cv_val_auc_mean,\n",
    "            cv_val_auc_std, \n",
    "            auc,\n",
    "            cv_val_loss_mean,\n",
    "            EPOCHS[0],\n",
    "            best_epoch_val_auc_mean,\n",
    "            best_epoch_val_loss_mean,\n",
    "            0.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(headers)):\n",
    "    print(headers[n]+\":\", log_items[n])\n",
    "    \n",
    "log_data = pd.DataFrame([log_items], columns=headers)\n",
    "log_df = log_df.append(log_data)\n",
    "log_df.to_csv(f'{LOG_PATH}experiment_log.csv', index=False)\n",
    "log_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Kaggle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n",
    "                 labeled=False, return_image_names=True)\n",
    "\n",
    "image_names = np.array([img_name.numpy().decode(\"utf-8\") \n",
    "                        for img, img_name in iter(ds.unbatch())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\n",
    "submission = submission.sort_values('image_name') \n",
    "submission.to_csv(f'{CV_TEST_PREDS_PATH}EB{EFF_NETS[0]}-{IMG_SIZES[0]}-{FOLDS}FCV-submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission.target,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
