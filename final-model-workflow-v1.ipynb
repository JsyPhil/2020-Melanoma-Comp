{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions Workflow Notebook v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of information, code and discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This notebook follows the 5 step process presented in the Chris Deotte \"How to compete with GPUs Workshop\" [here][1].\n",
    "2. Triple stratified KFold TFRecords used for image data is explained [here][2].\n",
    "3. Some code sections have been reused from AgentAuers' notebook [here][3]\n",
    "4. The advantage of using different input sizes is discussed [here][4]\n",
    "5. Use external data by changing the variables `INC2019` and `INC2018`.These variables respectively indicate whether to load last year 2019 data and/or year 2018 + 2017 data. These datasets are discussed [here][5]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/how-to-compete-with-gpus-workshop\n",
    "[2]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/165526\n",
    "[3]: https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once\n",
    "[4]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/160147\n",
    "[5]: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/164910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Initiatialise environment and import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Use if running in Kaggle environment\n",
    "#!pip install -q efficientnet >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KaggleDatasets if running in Kaggle environment\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "import os, re, math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "* DEVICE - is GPU or TPU\n",
    "* SEED - a different seed produces a different triple stratified kfold split.\n",
    "* FOLDS - number of folds. Best set to 3, 5, or 15 for conistent number of train and validation samples across folds\n",
    "* IMG_SIZES - These are the image sizes to use each fold\n",
    "* INC2019 - This includes the new half of the 2019 competition data. The second half of the 2019 data is the comp data from 2018 plus 2017\n",
    "* INC2018 - This includes the second half of the 2019 competition data which is the comp data from 2018 plus 2017\n",
    "* BATCH_SIZES - These are batch sizes for each fold. For maximum speed, it is best to use the largest batch size your GPU or TPU allows.\n",
    "* EPOCHS - These are maximum epochs. Note that each fold, the best epoch model is saved and used. So if epochs is too large, it won't matter. Consider early stopping in Callbacks.\n",
    "* EFF_NETS - These are the EfficientNets to use each fold. The number refers to the B. So a number of `0` refers to EfficientNetB0, and `1` refers to EfficientNetB1, etc.\n",
    "* WGTS - this should be `1/FOLDS` for each fold. This is the weight when ensembling the folds to predict the test set. If you want a weird ensemble, you can use different weights.\n",
    "* TTA - test time augmentation. Each test image is randomly augmented and predicted TTA times and the average prediction is used. TTA is also applied to OOF during validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image sizes used in the Efficientnet model:\n",
    "\n",
    "- efficientnet-b0-224 -cv train with 192\n",
    "- efficientnet-b1-240 -cv train with 256\n",
    "- efficientnet-b2-260 -cv train with 256\n",
    "- efficientnet-b3-300 -cv train with 384\n",
    "- efficientnet-b4-380 -cv train with 384\n",
    "- efficientnet-b5-456 -cv train with 512\n",
    "- efficientnet-b6-528 -cv train with 512\n",
    "- efficientnet-b7-600 -cv train with 768\n",
    "\n",
    "Better to train with a larger size and let the model scale down rather than up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "ENV = \"LOCAL\" #or \"KAGGLE\"\n",
    "\n",
    "# DEFAULT TO TPU TO ENSURE KAGGLE TPU COMPATIABILITY\n",
    "# https://www.kaggle.com/docs/tpu\n",
    "DEVICE = \"TPU\" #or \"GPU\"\n",
    "\n",
    "# WHICH IMAGE SIZES TO LOAD EACH FOLD\n",
    "# CHOOSE 128, 192, 256, 384, 512, 768 \n",
    "IMG_SIZES = 256\n",
    "\n",
    "# META DATA? YES=1 NO=0\n",
    "META = 1\n",
    "\n",
    "# INCLUDE OLD COMP DATA? YES=1 NO=0\n",
    "INC2019 = 0\n",
    "INC2018 = 1\n",
    "\n",
    "# UPSAMPLE MALIGNANT\n",
    "M1 = 1 #2020 malig\n",
    "M2 = 1 #ISIC malig\n",
    "M3 = 1 #2019 good malig\n",
    "M4 = 1 #2018 2017 malig - Malignant only images from HAM10000\n",
    "\n",
    "# COARSE DROPOUT\n",
    "DROP_FREQ = 0 # 0.75 # between 0 and 1\n",
    "DROP_CT = 0 # 8 # may slow training if CT>16\n",
    "DROP_SIZE = 0 # 0.2 # between 0 and 1\n",
    "\n",
    "# BATCH SIZE AND EPOCHS\n",
    "# TRY 8, 16, 32, 64, 128, 256. REDUCE IF OOM ERROR, HIGHER FOR TPUS\n",
    "BATCH_SIZES = 32\n",
    "EPOCHS = 35\n",
    "\n",
    "# METADATA ENSEMBLE\n",
    "META_ENSEMBLE = 1 # 1 = Yes, 0 = No\n",
    "META_WEIGHT = 0.03 # Optimal weight to use in Ensemble\n",
    "\n",
    "# WHICH EFFICIENTNET B? TO USE\n",
    "EFF_NETS = 1\n",
    "\n",
    "# TEST TIME AUGMENTATION STEPS\n",
    "TTA = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure environment to use TPUs, Multiple GPUs, Single GPU or just CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Could not connect to TPU\n",
      "Using default strategy for CPU and GPU\n",
      "Num GPUs Available:  2\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "REPLICAS: 2\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and GPU\")\n",
    "    \n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocess and file handling\n",
    "Preprocess has already been done and saved to TFRecords. Here we choose which size to load. We can use either 128x128, 192x192, 256x256, 384x384, 512x512, 768x768 by changing the `IMG_SIZES` variable in the preceeding code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file locations\n",
    "if ENV == 'KAGGLE':\n",
    "    #Use GCS if running in Kaggle environment\n",
    "    PATH = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(IMG_SIZES,IMG_SIZES))\n",
    "    PATH2 = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(IMG_SIZES,IMG_SIZES))\n",
    "    PATH3 = KaggleDatasets().get_gcs_path('malignant-v2-%ix%i'%(IMG_SIZES,IMG_SIZES))\n",
    "    files_train = np.sort(np.array(tf.io.gfile.glob(PATH + '/train*.tfrec')))\n",
    "    files_test  = np.sort(np.array(tf.io.gfile.glob(PATH + '/test*.tfrec')))\n",
    "       \n",
    "if ENV == 'LOCAL':  \n",
    "    # Use LDS if running in local environment\n",
    "    PATH = None; PATH2 = None; PATH3 = None\n",
    "    PATH = f\"./siim-isic-melanoma-classification/tfrecords{IMG_SIZES}\"\n",
    "    PATH2 = f\"./siim-isic-melanoma-classification/tfrecords{IMG_SIZES}Ext\"\n",
    "    PATH3 = f\"./siim-isic-melanoma-classification/tfrecords{IMG_SIZES}Mal\"\n",
    "    files_train = np.sort(np.array(tf.io.gfile.glob(PATH + '/train*.tfrec')))\n",
    "    files_test  = np.sort(np.array(tf.io.gfile.glob(PATH + '/test*.tfrec')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file locations\n",
    "if ENV == 'KAGGLE':\n",
    "    LOG_PATH = ''\n",
    "    CV_OOF_PREDS_PATH = ''\n",
    "    CV_TEST_PREDS_PATH = ''\n",
    "    CV_FOLDS_PATH = ''\n",
    "\n",
    "if ENV == 'LOCAL':\n",
    "    LOG_PATH = './logs/'\n",
    "    CV_OOF_PREDS_PATH = './cv_oof_preds/'\n",
    "    CV_TEST_PREDS_PATH = './cv_test_preds/'\n",
    "    CV_FOLDS_PATH = './cv_folds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File handling functions\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['target']\n",
    "\n",
    "def read_meta_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image_name'], example['sex'], example['age_approx'], example['anatom_site_general_challenge'], example['target']\n",
    "\n",
    "def read_meta_tfrecord_test(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image_name'], example['sex'], example['age_approx'], example['anatom_site_general_challenge']\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_name):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['image_name'] if return_image_name else 0\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream datasets into model when training, validating and predicting\n",
    "def get_dataset(files, augment = False, shuffle = False, repeat = False, \n",
    "                labeled=True, return_image_names=True, batch_size=16, dim=256,\n",
    "                droprate=0, dropct=0, dropsize=0):\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "        \n",
    "    if labeled: \n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n",
    "                    num_parallel_calls=AUTO)      \n",
    "    \n",
    "    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim,\n",
    "                                               droprate=droprate, dropct=dropct, dropsize=dropsize),\n",
    "                                               imgname_or_label), \n",
    "                num_parallel_calls=AUTO)\n",
    "    \n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream datasets into model when training, validating and predicting\n",
    "column_names = ['image_name', 'sex', 'age_approx', 'anatom', 'target']\n",
    "column_names_test = ['image_name', 'sex', 'age_approx', 'anatom']\n",
    "\n",
    "def get_meta(files, batch_size=16, test=0):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    if test:\n",
    "        ds = ds.map(lambda example: read_meta_tfrecord_test(example), num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_meta_tfrecord(example), num_parallel_calls=AUTO)      \n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "def meta_unbatch(files, test=0):\n",
    "    if test:\n",
    "        meta_np = np.array([[img_name.numpy().decode(\"utf-8\"),\n",
    "                  sex.numpy(),\n",
    "                  age.numpy(),\n",
    "                  anatomy.numpy()]\n",
    "                for img_name, sex, age, anatomy in iter(files.unbatch())])\n",
    "        meta_df = pd.DataFrame(data = meta_np, columns=column_names_test)\n",
    "\n",
    "    else:        \n",
    "        meta_np = np.array([[img_name.numpy().decode(\"utf-8\"),\n",
    "                  sex.numpy(),\n",
    "                  age.numpy(),\n",
    "                  anatomy.numpy(),\n",
    "                  target.numpy()] \n",
    "                for img_name, sex, age, anatomy, target in iter(files.unbatch())])\n",
    "        \n",
    "        meta_df = pd.DataFrame(data = meta_np, columns=column_names)\n",
    "        meta_df = meta_df.astype({\"target\": int})\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Augmentation\n",
    "This notebook uses rotation, sheer, zoom, shift augmentation. This notebook also uses horizontal flip, saturation, contrast, brightness augmentation similar to last years winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "ROT_ = 180.0\n",
    "SHR_ = 2.0\n",
    "HZOOM_ = 8.0\n",
    "WZOOM_ = 8.0\n",
    "HSHIFT_ = 8.0\n",
    "WSHIFT_ = 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(image, DIM=256, PROBABILITY = 0.75, CT = 8, SZ = 0.2):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "    \n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n",
    "    if (P==0)|(CT==0)|(SZ==0): return image\n",
    "    \n",
    "    for k in range(CT):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3]) \n",
    "        three = image[ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n",
    "            \n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n",
    "    image = tf.reshape(image,[DIM,DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, DIM=256):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = SHR_ * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n",
    "    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])\n",
    "\n",
    "def prepare_image(img, augment=True, dim=256, droprate=0, dropct=0, dropsize=0):    \n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if augment:\n",
    "        img = transform(img,DIM=dim)\n",
    "        if (droprate!=0)&(dropct!=0)&(dropsize!=0): \n",
    "            img = dropout(img, DIM=dim, PROBABILITY=droprate, CT=dropct, SZ=dropsize)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        #img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1)\n",
    "                      \n",
    "    img = tf.reshape(img, [dim,dim, 3])\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build Model\n",
    "This is a common model architecute. Consider experimenting with different backbones, custom heads, losses, and optimizers. Also consider inputing meta features into your CNN. Also consider different models to provide diversity of predictions which may benefit the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b1 (Functional) (None, 8, 8, 1280)        6575232   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 6,576,513\n",
      "Trainable params: 6,514,465\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fcdf42a5550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "\n",
    "def build_model(dim=128, ef=0, v=0):\n",
    "    inputs = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    \n",
    "    base = EFNS[ef](input_shape=(dim,dim,3),weights='noisy-student',include_top=False)\n",
    "    \n",
    "    x = base(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n",
    "    \n",
    "    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n",
    "    \n",
    "    if v:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Visualise model in use\n",
    "build_model(dim=IMG_SIZES,ef=EFF_NETS,v=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META DATA FUNCTION\n",
    "\n",
    "# Function to get meta data\n",
    "def meta_model(meta_train, meta_val):\n",
    "    L = 15\n",
    "    feat = ['sex','age_approx','anatom']\n",
    "    M = meta_train.target.mean()\n",
    "    te = meta_train.groupby(feat)['target'].agg(['mean','count']).reset_index()\n",
    "    te['ll'] = ((te['mean']*te['count'])+(M*L))/(te['count']+L)\n",
    "    del te['mean'], te['count']\n",
    "    meta_val = meta_val.merge( te, on=feat, how='left' )\n",
    "    meta_val['ll'] = meta_val['ll'].fillna(M)\n",
    "    return meta_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Schedule\n",
    "Thism is a common train schedule for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time. Consider changing the schedule and/or learning rates. Note how the learning rate max is larger with larger batches sizes. This is a good practice to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_CFG = dict(\n",
    "    lr_start   = 0.000006, \n",
    "    lr_max     = 0.00000145 * REPLICAS * BATCH_SIZES,\n",
    "    lr_min     = 0.000001,\n",
    "    lr_ramp_ep = 5,\n",
    "    lr_sus_ep  = 0,\n",
    "    lr_decay   = 0.85\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(ts_cfg, batch_size=8):\n",
    "    lr_start   = ts_cfg['lr_start']\n",
    "    lr_max     = ts_cfg['lr_max']\n",
    "    lr_min     = ts_cfg['lr_min']\n",
    "    lr_ramp_ep = ts_cfg['lr_ramp_ep']\n",
    "    lr_sus_ep  = ts_cfg['lr_sus_ep']\n",
    "    lr_decay   = ts_cfg['lr_decay']\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    \n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Training Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick test and visualisation of learning rate schedule\n",
    "# Important not to have a high learning rate which would destroy the pre-trained parameters\n",
    "lr_start   = TS_CFG['lr_start']\n",
    "lr_max     = TS_CFG['lr_max']\n",
    "lr_min     = TS_CFG['lr_min']\n",
    "lr_ramp_ep = TS_CFG['lr_ramp_ep']\n",
    "lr_sus_ep  = TS_CFG['lr_sus_ep']\n",
    "lr_decay   = TS_CFG['lr_decay']\n",
    "\n",
    "def lrfn_sched(epoch):\n",
    "    if epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "\n",
    "    return lr\n",
    "\n",
    "# Visualise learning rate schedule\n",
    "rng = [i for i in range(20)]\n",
    "y = [lrfn_sched(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "plt.xlabel('epoch', size=14); plt.ylabel('learning rate', size=14)\n",
    "plt.title('Training Schedule', size=16); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.51\n",
      "Weight for class 1: 35.24\n"
     ]
    }
   ],
   "source": [
    "# Analyze class imbalance in the targets\n",
    "# Number of positive samples in training data: 470 (1.77% of total 33126)\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / (33126-470))*(33126)/2.0 \n",
    "weight_for_1 = (1 / 470)*(33126)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variables `VERBOSE` and `DISPLOY_PLOT` below to determine what output you want displayed. The variable `VERBOSE=1 or 2` will display the training and validation loss and auc for each epoch as text. The variable `DISPLAY_PLOT` shows this information as a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "#### Image Size 256 with EfficientNet B1 and batch_size 64\n",
      "#### DropoutFreq 0 Count 0 Size 0\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\n",
    "VERBOSE = 1\n",
    "DISPLAY_PLOT = True\n",
    "    \n",
    "# DISPLAY FOLD INFO\n",
    "if DEVICE=='TPU':\n",
    "    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "print('#'*25)\n",
    "print(f'#### Image Size {IMG_SIZES} with EfficientNet B{EFF_NETS} and batch_size {BATCH_SIZES*REPLICAS}')\n",
    "print(f'#### DropoutFreq {DROP_FREQ} Count {DROP_CT} Size {DROP_SIZE}')\n",
    "print('#'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAIN INDEX LIST\n",
    "idxT = np.asarray([x for x in range(0,15,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "#### Using 2018+2017 external data\n",
      "#### Upsample MALIG-1 data (2020 comp)\n",
      "#### Upsample MALIG-2 data (ISIC website)\n",
      "#### Upsample MALIG-3 data (2019 comp)\n",
      "#### Upsample MALIG-4 data (2018 2017 comp)\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN SET FROM VARIOUS SELECTED SOURCES\n",
    "print('#'*25)\n",
    "files_train = tf.io.gfile.glob([PATH + '/train%.2i*.tfrec'%x for x in idxT])\n",
    "if INC2019:\n",
    "    files_train += tf.io.gfile.glob([PATH2 + '/train%.2i*.tfrec'%x for x in idxT*2+1])\n",
    "    print('#### Using 2019 external data')\n",
    "if INC2018:\n",
    "    files_train += tf.io.gfile.glob([PATH2 + '/train%.2i*.tfrec'%x for x in idxT*2])\n",
    "    print('#### Using 2018+2017 external data')\n",
    "if M1:\n",
    "    files_train += tf.io.gfile.glob([PATH3 + '/train%.2i*.tfrec'%x for x in idxT])\n",
    "    print('#### Upsample MALIG-1 data (2020 comp)')\n",
    "if M2:\n",
    "    files_train += tf.io.gfile.glob([PATH3 + '/train%.2i*.tfrec'%x for x in idxT+15])\n",
    "    print('#### Upsample MALIG-2 data (ISIC website)')\n",
    "if M3:\n",
    "    files_train += tf.io.gfile.glob([PATH3 + '/train%.2i*.tfrec'%x for x in idxT*2+1+30])\n",
    "    print('#### Upsample MALIG-3 data (2019 comp)')\n",
    "if M4:\n",
    "    files_train += tf.io.gfile.glob([PATH3 + '/train%.2i*.tfrec'%x for x in idxT*2+30])\n",
    "    print('#### Upsample MALIG-4 data (2018 2017 comp)')        \n",
    "np.random.shuffle(files_train)\n",
    "print('#'*25)\n",
    "\n",
    "# CREATE TEST SET\n",
    "files_test = np.sort(np.array(tf.io.gfile.glob(PATH + '/test*.tfrec')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "Weight for class 0: 0.57\n",
      "Weight for class 1: 3.89\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# DETERMINE CLASS WEIGHTS FROM TRAINING DATASET\n",
    "# Analyze class imbalance in the targets\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "ft = get_dataset(files_train, augment=False, repeat=False, dim=IMG_SIZES,\n",
    "        labeled=True, return_image_names=True)\n",
    "ott = np.array([target.numpy() for img, target in iter(ft.unbatch())])\n",
    "tot = len(ott)\n",
    "counts = np.bincount(ott[:])\n",
    "\n",
    "weight_for_0 = (1 / counts[0])*(tot)/2.0 \n",
    "weight_for_1 = (1 / counts[1])*(tot)/2.0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "print('#'*25)\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('#'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b1 (Functional) (None, 8, 8, 1280)        6575232   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 6,576,513\n",
      "Trainable params: 6,514,465\n",
      "Non-trainable params: 62,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD MODEL\n",
    "K.clear_session()\n",
    "with strategy.scope():\n",
    "    model = build_model(dim=IMG_SIZES,ef=EFF_NETS,v=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS - SAVE MODEL WEIGHTS AT EACH EPOCH\n",
    "WEIGHTS_FILE = f'{CV_FOLDS_PATH}EB{EFF_NETS}-{IMG_SIZES}-final.h5'\n",
    "svw = tf.keras.callbacks.ModelCheckpoint(WEIGHTS_FILE,\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=True)\n",
    "\n",
    "callback_list = [svw, get_lr_callback(TS_CFG, BATCH_SIZES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/35\n",
      "WARNING:tensorflow:From /home/phil/anaconda3/envs/tf37/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 301 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 301 all-reduces with algorithm = nccl, num_packs = 1\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.5776 - auc: 0.8121\n",
      "Epoch 00001: saving model to ./cv_folds/EB1-256-final.h5\n",
      "748/748 [==============================] - 242s 323ms/step - loss: 0.5776 - auc: 0.8121\n",
      "Epoch 2/35\n",
      "143/748 [====>.........................] - ETA: 3:13 - loss: 0.4231 - auc: 0.8931"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "# Use // on REPLICAS to divide with integral result (discard remainder)\n",
    "print('Training...')\n",
    "history = model.fit(\n",
    "    get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n",
    "                dim=IMG_SIZES, batch_size = BATCH_SIZES,\n",
    "                droprate=DROP_FREQ, dropct=DROP_CT, dropsize=DROP_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = callback_list, \n",
    "    steps_per_epoch=count_data_items(files_train)/BATCH_SIZES//REPLICAS,\n",
    "    class_weight=class_weight,\n",
    "    verbose=VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELOAD WEIGHTS\n",
    "print('#'*25)\n",
    "print('Loading weights...')\n",
    "model.load_weights(WEIGHTS_FILE)\n",
    "print('Done.')\n",
    "print('#'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT TEST USING TTA\n",
    "preds = np.zeros((count_data_items(files_test),1))\n",
    "\n",
    "print('#'*25)\n",
    "print('Predicting Test with TTA...')\n",
    "ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n",
    "                     repeat=True,shuffle=False,dim=IMG_SIZES,batch_size=BATCH_SIZES*4,\n",
    "                     droprate=DROP_FREQ, dropct=DROP_CT, dropsize=DROP_SIZE)\n",
    "\n",
    "ct_test = count_data_items(files_test)\n",
    "print('Number of test samples:', ct_test)\n",
    "STEPS = TTA * ct_test/BATCH_SIZES/4/REPLICAS\n",
    "print('Number of steps in prediction:', STEPS)\n",
    "pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,]\n",
    "pred = np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1)\n",
    "preds[:,0] += pred\n",
    "print('#'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE USING METADATA PROBABILITY MODEL\n",
    "preds_meta_ls = np.zeros((count_data_items(files_test),1))\n",
    "\n",
    "if META_ENSEMBLE:\n",
    "    print('#'*25)\n",
    "    print(\"Performing ensemble of image predictions with metadata probability model using meta weight of:\", META_WEIGHT)\n",
    "\n",
    "    dsp_ft = get_meta(files_train)\n",
    "    meta_train = meta_unbatch(dsp_ft)\n",
    "\n",
    "    ds_test_meta = get_meta(files_test, BATCH_SIZES, test=1)\n",
    "    meta_test = meta_unbatch(ds_test_meta, test=1)\n",
    "    meta_test = meta_model(meta_train, meta_test)\n",
    "\n",
    "    meta_test['preds'] = pred\n",
    "    meta_test['preds_meta'] = META_WEIGHT*meta_test.ll + (1-META_WEIGHT)*meta_test.preds\n",
    "\n",
    "    preds_meta = np.array([meta_test.preds_meta]).T\n",
    "    preds_meta_ls += preds_meta\n",
    "    print('#'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING\n",
    "COMPLETED_EPOCHS = len(history.history['loss'])\n",
    "if DISPLAY_PLOT:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(np.arange(COMPLETED_EPOCHS),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
    "    x = np.argmax( history.history['auc'] )\n",
    "    y = np.max( history.history['auc'] )\n",
    "    xdist = plt.xlim()[1] - plt.xlim()[0]\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#1f77b4')\n",
    "    plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
    "    plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n",
    "    plt.legend(loc=2)\n",
    "    plt2 = plt.gca().twinx()\n",
    "    plt2.plot(np.arange(COMPLETED_EPOCHS),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "    x = np.argmin( history.history['loss'] )\n",
    "    y = np.min( history.history['loss'] )\n",
    "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "    plt.scatter(x,y,s=200,color='#d62728')\n",
    "    plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "    plt.ylabel('Loss',size=14)\n",
    "    plt.title('Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i, M1=%i, M2=%i, M3=%i, M4=%i\\n\\\n",
    "    batch_size %i, dropout_freq=%.2f count=%i size=%.3f'%\n",
    "            (IMG_SIZES,EFF_NETS,INC2019,INC2018,M1,M2,M3,\n",
    "             M4,BATCH_SIZES*REPLICAS,DROP_FREQ,DROP_CT,DROP_SIZE),size=18)\n",
    "    plt.legend(loc=3)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Kaggle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES,\n",
    "                 labeled=False, return_image_names=True)\n",
    "\n",
    "image_names = np.array([img_name.numpy().decode(\"utf-8\") \n",
    "                        for img, img_name in iter(ds.unbatch())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\n",
    "submission = submission.sort_values('image_name') \n",
    "submission.to_csv(f'{CV_TEST_PREDS_PATH}EB{EFF_NETS}-{IMG_SIZES}-FINAL-submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_meta = pd.DataFrame(dict(image_name=image_names, target=preds_meta_ls[:,0]))\n",
    "submission_meta = submission_meta.sort_values('image_name') \n",
    "submission_meta.to_csv(f'{CV_TEST_PREDS_PATH}EB{EFF_NETS}-{IMG_SIZES}-FINAL-META-submission.csv', index=False)\n",
    "submission_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT DISTRIBUTION WITHOUT META\n",
    "plt.hist(submission.target,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT DISTRIBUTION WITH META\n",
    "plt.hist(submission_meta.target,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
